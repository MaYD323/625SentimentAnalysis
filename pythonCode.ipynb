{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sa\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pd.options.mode.chained_assignment = None\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('yelp_review_polarity_csv/train.csv', sep = ',', names =  ['attitude','text'])\n",
    "test = pd.read_csv('yelp_review_polarity_csv/test.csv', sep = ',', names =  ['attitude','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unification(data_cleaned):\n",
    "    print(\"uni\")\n",
    "    #Making all letters lowercase\n",
    "    data_cleaned['text'] = data_cleaned['text'].apply(lambda x: x.lower())\n",
    "    #Removing Punctuation, Symbols\n",
    "    data_cleaned['text'] = data_cleaned['text'].str.replace('[^\\w\\s]',' ')\n",
    "    #Removing Stop Words\n",
    "    stop_words = stopwords.words('english')\n",
    "    for i in range(len(data_cleaned)):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word not in stop_words:\n",
    "                text+=' '+word\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    \n",
    "    return data_cleaned\n",
    "\n",
    "def lemmatisation(data_cleaned):\n",
    "    print(\"lemma\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(data_cleaned)):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            text+=' '+lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word),pos = 'a'), pos = 'v')\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "\n",
    "word_dict = {}\n",
    "def removing_rare_words(data_cleaned, num_to_ignore = 5000):\n",
    "    print(\"removing\")\n",
    "    word_dict = {}\n",
    "    for i in range(len(data_cleaned)):\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word in word_dict:\n",
    "                word_dict[word]+=1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "    rarest_words = sorted(list(word_dict.keys()), key = lambda x:word_dict[x])[:num_to_ignore]\n",
    "\n",
    "    for i in range(len(data_cleaned)):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word not in rarest_words:\n",
    "                text+=' '+word\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "\n",
    "def removing_rare_words_test(data_cleaned, num_to_ignore = 5000):\n",
    "    rarest_words = sorted(list(word_dict.keys()), key = lambda x:word_dict[x])[:num_to_ignore]\n",
    "    for i in range(len(data_cleaned)):\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word not in rarest_words:\n",
    "                text+=' '+word\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "    \n",
    "def data_cleaning(data_cleaned, num_to_ignore = 5000):\n",
    "    return removing_rare_words(lemmatisation(unification(data_cleaned)), num_to_ignore)\n",
    "\n",
    "def data_cleaning_test(data_cleaned, num_to_ignore = 5000):\n",
    "    return removing_rare_words_test(lemmatisation(unification(data_cleaned)), num_to_ignore)\n",
    "    \n",
    "analyzer = sa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "lemma\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "removing\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n"
     ]
    }
   ],
   "source": [
    "data_cleaned = data_cleaning(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uni\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "lemma\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "test_data_cleaned = data_cleaning_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.to_csv(\"cleaned_data/cleaned_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned.to_csv(\"cleaned_data/cleaned_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.8679210526315789\n",
      "lsvm using count vectors accuracy 0.9308684210526316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg count vectors accuracy 0.9353421052631579\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(data_cleaned['text'])\n",
    "\n",
    "X_train_count = count_vect.transform(data_cleaned['text'])\n",
    "y_train = data_cleaned[\"attitude\"]\n",
    "X_val_count = count_vect.transform(test_data_cleaned['text']) #validation\n",
    "y_val =test_data_cleaned[\"attitude\"]\n",
    "\n",
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred = nb.predict(X_val_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_count, y_train)\n",
    "y_pred = logreg.predict(X_val_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.8182105263157895\n",
      "svm using tfidf accuracy 0.8583157894736843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg tfidf accuracy 0.8723947368421052\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=500, analyzer='word',ngram_range=(1,3))\n",
    "tfidf.fit(data_cleaned['text'])\n",
    "\n",
    "X_train_tfidf = tfidf.transform(data_cleaned['text'])\n",
    "y_train = data_cleaned[\"attitude\"]\n",
    "X_val_tfidf = tfidf.transform(test_data_cleaned['text']) #validation\n",
    "y_val =test_data_cleaned[\"attitude\"]\n",
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_val_tfidf)\n",
    "print(\"naive bayes tfidf accuracy %s\" %  accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_tfidf, y_train)\n",
    "y_pred = lsvm.predict(X_val_tfidf)\n",
    "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 3: logistic regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_val_tfidf)\n",
    "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
